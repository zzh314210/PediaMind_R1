# train_lora.py (æœ€ç»ˆä¿®æ­£ç‰ˆ)

import os
import json
import torch
import datetime
import logging
import sys
from dataclasses import dataclass, field
from typing import Optional

from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    TrainerCallback,
    HfArgumentParser
)
from peft import (
    get_peft_model,
    LoraConfig,
    TaskType
)

# ========== æ—¥å¿—è®¾ç½® ==========
if int(os.environ.get("RANK", 0)) == 0:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
logger = logging.getLogger(__name__)

# ========== å‚æ•°å®šä¹‰ ==========
@dataclass
class ModelArguments:
    model_name_or_path: str = field(metadata={"help": "é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„æˆ–Hugging Faceæ¨¡å‹æ ‡è¯†ç¬¦"})

@dataclass
class DataArguments:
    data_path: str = field(metadata={"help": "è®­ç»ƒæ•°æ® .jsonl æ–‡ä»¶çš„è·¯å¾„"})
    max_seq_length: int = field(default=1024, metadata={"help": "Tokenizeåçš„æœ€å¤§åºåˆ—é•¿åº¦"})

# ========== æ˜¾å­˜ç›‘æ§å›è°ƒ ==========
def log_memory(prefix=""):
    allocated = torch.cuda.memory_allocated() / 1024**3
    reserved = torch.cuda.memory_reserved() / 1024**3
    logger.info(f"[{prefix}] ğŸ’¾ GPUæ˜¾å­˜ -> å·²åˆ†é…: {allocated:.2f} GB | å·²é¢„ç•™: {reserved:.2f} GB")

class MemoryLoggerCallback(TrainerCallback):
    def on_step_end(self, args, state, control, **kwargs):
        if state.is_world_process_zero and state.global_step % args.logging_steps == 0:
            log_memory(f"Step {state.global_step}")

def main():
    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    if training_args.local_rank == 0:
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_args.model_name_or_path}")
        logger.info(f"æ‰€æœ‰è®­ç»ƒå‚æ•°: {training_args}")

    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_args.model_name_or_path,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True
    )
    
    if training_args.local_rank == 0:
        logger.info("æ­£åœ¨é…ç½® LoRA ...")
    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=["q_proj", "v_proj", "up_proj", "down_proj"]
    )
    model = get_peft_model(model, lora_config)
    
    model.enable_input_require_grads()
    if training_args.local_rank == 0:
        model.print_trainable_parameters()

    if training_args.local_rank == 0:
        logger.info(f"æ­£åœ¨ä» {data_args.data_path} åŠ è½½æ•°æ®...")
        
    with open(data_args.data_path, "r", encoding="utf-8") as f:
        raw_data = [json.loads(line) for line in f]

    def format_prompt(example):
        system_prompt = """
            ä½ æ˜¯ä¸€ä¸ªè‚²å©´ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯å›ç­”å®¶é•¿çš„è‚²å©´æé—®ï¼Œä½ éœ€è¦æ ¹æ®å©´å¹¼å„¿çš„æ°”è´¨è¿›è¡Œåˆ†æï¼Œç»™å‡ºåˆç†çš„åˆ†æè¿‡ç¨‹å¹¶ç»™å‡ºç­”æ¡ˆ
            ä½ éœ€è¦é¦–å…ˆæ€è€ƒå®¶é•¿çš„æé—®ï¼Œç”¨è‡ªå·±çš„è¯é‡æ–°è¡¨è¿°å®¶é•¿æ­£é¢ä¸´ä»€ä¹ˆæ ·çš„è‚²å©´é—®é¢˜
            æ¥ç€ä½ éœ€è¦åˆ†æå©´å„¿çš„æ°”è´¨ç‰¹å¾ï¼Œå¹¶åˆ†æå©´å„¿å½“å‰çš„è¡Œä¸ºä½“ç°äº†ä»€ä¹ˆæ°”è´¨ç‰¹å¾ï¼Œæé†’è‡ªå·±å¯ä»¥è®©å®¶é•¿ä¸ç”¨æ‹…å¿ƒï¼Œå®‰æŠšä¸€ä¸‹ï¼Œæ°”è´¨åˆ†ææ”¾åœ¨<temperment></temperment>æ ‡ç­¾ä¹‹é—´
            ç„¶åä½ éœ€è¦åˆ†æå©´å„¿å½“å‰æ°”è´¨ç‰¹å¾å¯¹åº”çš„å…»è‚²ç­–ç•¥ï¼Œæˆ‘åº”è¯¥æ€ä¹ˆæ ·åšï¼Œé¿å…å’Œå©´å„¿è‡ªèº«æ€§æ ¼å†²çªçš„åšæ³•ã€‚å…»è‚²ç­–ç•¥æ”¾åœ¨<strategy></strategy>ä¹‹é—´
            ä»¥ä¸Šæ‰€æœ‰åˆ†ææ”¾åœ¨<think></think>æ ‡ç­¾ä¹‹é—´
            æœ€åç»™å®¶é•¿å½“å‰å»ºè®®æ€ä¹ˆåšï¼Œæ—¢è¦è´´åˆå©´å„¿æ°”è´¨ï¼Œåˆè¦ç´§å¯†å…³è”ç”¨æˆ·çš„é—®é¢˜ï¼Œç»™å‡ºé’ˆå¯¹æ€§çš„è€Œä¸æ˜¯æ³›æ³›è€Œè°ˆçš„ç­”æ¡ˆã€‚å›ç­”å®Œä»¥åå†ç®€çŸ­æ€»ç»“ï¼Œç»™å®¶é•¿ä¿¡å¿ƒã€‚ä»¥ä¸Šæ”¾åœ¨<answer></answer>æ ‡ç­¾ä¹‹é—´
            """
        messages = [
            {"role": "system", "content": system_prompt},
            # {"role": "user", "content": example["question"]},
            # {"role": "assistant", "content": example["answer"]}
            {"role": "user", "content": example["prompt"]},
            {"role": "assistant", "content": example["response"]}
        ]
        return {"text": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}

    dataset = Dataset.from_list([format_prompt(x) for x in raw_data])

    # ==================== å‡½æ•°ä¿®æ­£åŒºåŸŸ V3 ====================
    def tokenize_fn(examples):
        im_start_token_id = tokenizer.convert_tokens_to_ids("<|im_start|>")
        assistant_token_id = tokenizer.convert_tokens_to_ids("assistant")
        
        # æ ¸å¿ƒæ”¹åŠ¨ï¼šé‡æ–°å¯ç”¨åœ¨ map é˜¶æ®µçš„å¡«å……ã€‚
        # è¿™ç¡®ä¿äº†é€å…¥ DataCollator ä¹‹å‰ï¼Œæ¯ä¸ªæ ·æœ¬çš„ 'input_ids', 'attention_mask', 
        # å’Œæˆ‘ä»¬æ‰‹åŠ¨åˆ›å»ºçš„ 'labels' éƒ½å…·æœ‰ç›¸åŒçš„é•¿åº¦ã€‚
        tokenized_output = tokenizer(
            examples["text"],
            max_length=data_args.max_seq_length,
            truncation=True,
            padding="max_length", # <-- æ”¹å› "max_length"
        )
        input_ids_list = tokenized_output["input_ids"]
        labels_list = []

        for input_ids in input_ids_list:
            # ç°åœ¨ input_ids å·²ç»æ˜¯è¢«å¡«å……è¿‡çš„åˆ—è¡¨äº†
            labels = list(input_ids)
            assistant_start_index = -1
            try:
                # æ‰¾åˆ° assistant éƒ¨åˆ†çš„èµ·å§‹ç‚¹
                # ä»1å¼€å§‹å¾ªç¯ï¼Œé¿å… input_ids[i-1] è¶Šç•Œ
                for i in range(1, len(input_ids)):
                    if input_ids[i-1] == im_start_token_id and input_ids[i] == assistant_token_id:
                        assistant_start_index = i - 1 # assistantéƒ¨åˆ†çš„èµ·å§‹ä½ç½®æ˜¯ <|im_start|>
                        break
            except IndexError:
                # åœ¨æå°‘æ•°æƒ…å†µä¸‹ï¼Œå¦‚æœåºåˆ—è¢«æˆªæ–­å¾—éå¸¸çŸ­ï¼Œå¯èƒ½ä¼šå‘ç”Ÿç´¢å¼•é”™è¯¯
                assistant_start_index = -1

            if assistant_start_index != -1:
                # å±è”½ assistant ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
                for i in range(assistant_start_index):
                    labels[i] = -100
            else:
                # å®‰å…¨æªæ–½: å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå±è”½æ‰€æœ‰æ ‡ç­¾
                if training_args.local_rank == 0:
                    logger.warning(
                        "åœ¨ä¸€æ¡æ•°æ®ä¸­æœªæ‰¾åˆ° assistant æ ‡è¯†ï¼Œå°†å±è”½æ‰€æœ‰æ ‡ç­¾ã€‚è¯·æ£€æŸ¥æ•°æ®æ ¼å¼æˆ–æˆªæ–­é•¿åº¦ã€‚"
                        f" (åºåˆ—é•¿åº¦: {len(input_ids)})"
                    )
                for i in range(len(labels)):
                    labels[i] = -100
            
            labels_list.append(labels)
        
        tokenized_output["labels"] = labels_list
        return tokenized_output
    # =========================================================

    tokenized_dataset = dataset.map(tokenize_fn, batched=True, remove_columns=["text"], num_proc=os.cpu_count() // 2)
    if training_args.local_rank == 0:
        logger.info(f"æ•°æ®å¤„ç†å®Œæˆï¼Œæ•°æ®é›†å¤§å°: {len(tokenized_dataset)}")
        logger.info("æ•°æ®æ ·æœ¬é¢„è§ˆ (åˆ—):")
        print(tokenized_dataset.column_names)

    trainer = Trainer(
        model=model,
        tokenizer=tokenizer, 
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),
        callbacks=[MemoryLoggerCallback()]
    )

    if training_args.local_rank == 0:
        logger.info(f"ğŸš€ è®­ç»ƒå¼€å§‹ï¼å°†åœ¨ {torch.cuda.device_count()} å¼ GPUä¸Šè¿›è¡Œè®­ç»ƒã€‚")
    
    torch.cuda.empty_cache()
    trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)

    # if training_args.local_rank == 0:
    #     logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨ä¿å­˜ LoRA adapter åˆ°: {training_args.output_dir}")
    #     model.save_pretrained(training_args.output_dir)
    #     tokenizer.save_pretrained(training_args.output_dir)
    #     logger.info("æ¨¡å‹ä¿å­˜å®Œæ¯•ï¼")

    if training_args.local_rank == 0:
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ­£åœ¨åˆå¹¶ LoRA æƒé‡å¹¶ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {training_args.output_dir}")
        merged_model = model.merge_and_unload()  # åˆå¹¶ LoRA åˆ°åŸºç¡€æ¨¡å‹
        merged_model.save_pretrained(training_args.output_dir)
        tokenizer.save_pretrained(training_args.output_dir)
        logger.info("åˆå¹¶æ¨¡å‹ä¿å­˜å®Œæ¯•ï¼")

if __name__ == "__main__":
    main()